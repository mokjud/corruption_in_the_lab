---
title: "Monte Carlo simulations to estimate power"
author: "Anna Fedor"
editor_options: null
output:
  word_document:
    toc: yes
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    df_print: paged
    toc: yes
chunk_output_type: console
---

# Description

```{r Setup, include=FALSE}

rm(list=ls())

library(knitr)
library(rmarkdown)
library(dplyr)
library(stats)
library(ggplot2)

knitr::opts_chunk$set(include=TRUE, echo = FALSE, message = FALSE, warning = FALSE)

# Parameters

samplesize = c(36) # sample sizes to test (the number of participants in each group)
simulations = 1 # the number of simulations with a given sample size
partner_effect = 0.5 # the proportion of effect that partner type is responsible for 

t = 20 # the number of rounds (trials) each participant plays
d = 6 # the number of sides of the dice
datafile = "C:/Users/fedor/OneDrive/Documents/DOKUMENTUMOK/Corruption.Project/GitHub/corruption_in_the_lab/power_simulations/Wouda.etal_data.and.script/Wouda_data.RData"
setwd("C:/Users/fedor/OneDrive/Documents/DOKUMENTUMOK/Corruption.Project/GitHub/corruption_in_the_lab/power_simulations/RESULTS/")
seed = "2019-10-01 20:13:36" #Sys.time() # random seed

```

We used this script to estimate the necessary sample size for our experiments to reach enough power. Our goal was to achieve **at least 90% power** on all of our Wilcoxon signed rank U tests. We simulated participant behavior based on our assumptions and tested the outcome with a series of sample sizes. We ran **`r simulations` simulations with each sample size** and calculated power for a given sample size and a given comparison as the percentage of simulations where the Wilcoxon signed rank U test was significant, i.e., detected a true difference. We decided to choose the smallest sample size with which we could achieve the desired power on all of our tests. The random seed for the simulations was set to be the then current date and time: `r seed`.

# Simulating player A behavior

We planned to test our participants in four conditions with the sequential dyadic die-rolling task previously used by Weisel & Shalvi, 2015. The main difference is that in our experiment, player A will be simulated by the computer, although player B will be lead to believe that he is playing with a real person. Player A will be either "honest" or "dishonest". The reported values of honest player As will be sampled from a uniform distribution between 1 and 6. The reported values of dishonest player As will be sampled from the values reported by participants who were in the role of player A in Wouda et al., 2017, Study 2, High behavioral norm group. Sampling will be done for each player, independently.

The original authors sent us their data and script with which they calculated their statistics. In their experiment, participants's norm was manipulated by showing them results from previous experiments: either from an experiment where participants cheated quite often (High behavioral norm group) or from an experiment where participants cheated less often (Low behavioral norm group). The manipulation affected participant behavior: Participants in the high behavioral norm group reported higher values more often and reported double rolls more often than participants in the low behavioral norm group, see the figures below.

```{r Load data from Wouda et al. 2017}

load(datafile)

# Distribution of values reported by player A
valueA_distr_low <- table(a_T2_all_trials$PlayerA) # in the low behavioral treatment group
valueA_distr_high <- table(a_T9_all_trials$PlayerA) # in the high behavioral treatment group

Low <- valueA_distr_low/sum(valueA_distr_low)
High <- valueA_distr_high/sum(valueA_distr_high)

barplot(rbind(Low, High), beside=TRUE, 
        legend.text=TRUE,
        args.legend = c(x="topleft"),
        xlab = "Reported values by player As",
        ylab = "Proportion")

# Probability of reporting doubles by player B
double_count_low <- a_T2_last_trial$total_doubles # in the low behavioral treatment group
double_count_high <- a_T9_last_trial$total_doubles # in the high behavioral treatment group

double_prob_low <- sum(double_count_low) / (length(double_count_low)*20) # the probability of reporting doubles in the low behavioral norm group
double_prob_high <- sum(double_count_high) / (length(double_count_high)*20) # the probability of reporting doubles in the high behavioral norm group

boxplot(double_count_low, double_count_high, 
        names = c("Low", "High"), 
        ylab = "The number of doubles/dyad",
        xlab = "Behavioral treatment")

```

The distribution of values reported by player As in the high behavioral norm group was the following. We used this distribution to sample values for our simulated dishonest player As:

```{r Distribution of reported values by simulated dishonest player As}
kable(valueA_distr_high)
```

# Simulating player B behavior

For the estimation of power we simulated the behavior of player Bs too. For this, we had to guess what values would player Bs report in our control group and three experimental manipulation groups. Player Bs can only increase their payoff by reporting doubles: if they cheated, they would report more doubles than expected by chance. Apart from doubles, there is no point for player Bs to report higher values than what they actually rolled, except, if they intend to signal to player As, trying to convince them to cheat. We decided not to model this behavior for the purposes of power calculations.

We simulated player B behavior in the following way. For each of our participant groups, we estimated the probability with which player Bs would report doubles. Then we randomly assigned each roll as double or not double based on this probability. If a roll was a double, player B's reported value was the same as player A's value; if a roll was not a double, player B's reported value was sampled from 1 to 6 with a uniform distribution, excluding player A's value. For estimating the probability of reporting doubles, we used data from Wouda et al., 2017, Study 2, again. We chose this experiment because this had a manipulation that we assumed affected participant behavior similarly as our intended manipulations would do. 

## Simple game with honest partner (control group)

For our simple game with honest partner the probability of reporting a double was taken from the Low behavioral norm group: it was `r double_prob_low` (see their Table 1). This could potentially overestimate the probability of reporting doubles in our experiment. Differences that might lead to overestimation:

- Their participants were used to participating in economic studies, which supposedly increases the tendency to cheat. Most of our participants probably did not participate in any behavioral experiments before.
- Despite the norm manipulation, their player As still cheated by reporting higher values more often than expected by chance, which in turn might have influenced player B behavior to cheat too. Our simulated player As will be perfectly "honest", which might discourage cheating for player Bs too.
- The norm manipulation probably affected player B behavior by its own right. Although it decreased cheating compared to the high behavioral norm group, it might have increased cheating compared to no manipulation at all (which was not tested). The figures that participants studied before the experiment showed data from Study 1, where participants reported 30% of doubles, which is still higher than expected by chance.

Overestimating the probability of reporting doubles might lead to overestimating power for a given sample size with the one sample Wilcoxon signed rank U test - however, we did not worry about this, since the sample size necessary for two-sample Wilcoxon tests will be much higher, i.e., the one sample test won't be the one to define our sample size anyway.

## Charity game with dishonest partner (double manipulation group)

In our view, player B behavior in Wouda et al.'s Study 2 was afftected in two ways:

1. By the experimental manipulation of showing them figures about the results of previous experiments
2. By the behavior (reported values) of player A, which was also affected by the experimental manipulation

In other words, player B in the high behavioral norm group was "encouraged" to cheat not only by the experimental manipulation but also by the behavior of player A. 

In our charity game with dishonest partner the case is similar: player A cheats more than in the simple game with honest partner and also, there is the effect of the experimental manipulation of donating to a charity. The difference between honest and dishonest player A behavior in our case is higher than the difference between player A behavior in the high and low behavioral norm group in the Wouda et al. experiment, which could lead to underestimation of the effect, what is a safe thing to do when estimating power. As for the experimental manipulation, we have no way to guess how much charity would affect behavior in this experiment, so we just suppose that the effect would be similar to that of the norm manipulation in Wouda et al.'s experiment.

For these reasons we took the probability of reporting doubles from the High behavioral norm group: `r double_prob_high`. (The value of 67% reported in Wouda et al., 2017, Table 2 must be a typo.)

## Single manipulation groups

If we assume that our manipulations of partner honesty and game type have an added effect, then the probability of reporting doubles in our single manipulation groups (simple game with dishonest partner and charity game with honest partner) should be between that of the control group and the double manipulation group. For our power simulations we assumed that the effects of the manipulations are similar: both manipulations are responsible for **`r partner_effect*100`%** of the difference between the control group and the double manipulation group.

We note that equal effects yield the highest power on all tests and might potentially lead to underestimation of sample size. If it turns out that one of the manipulations, e.g., partner honesty has a stronger effect, then we would have underestimated power for the SH-SD and CH-CD comparisons and overestimated power for the SH-CH and SD-CD comparisons, thus effect size would be too small for the latter to reach the desired power.

## Summary

A completely honest player B would report doubles with the probability of `r 1/d`. In our power simulations we estimated that player Bs would report doubles in the different experimental groups with the following probabilities:

* Simple game with honest partner:  `r double_prob_low`
* Simple game with dishonest partner: `r double_prob_low + (double_prob_high - double_prob_low) * partner_effect`
* Charity game with honest partner: `r double_prob_low + (double_prob_high - double_prob_low) * (1-partner_effect)`
* Charity game with dishonest partner: `r double_prob_high`

``` {r Simulations, include=FALSE}

set.seed(seed)

POWERS <- data.frame(
  sample_size = samplesize,
  SH = vector(mode="numeric", length=length(samplesize)),
  SD = vector(mode="numeric", length=length(samplesize)),
  CH = vector(mode="numeric", length=length(samplesize)),
  CD = vector(mode="numeric", length=length(samplesize)),
  SH_SD = vector(mode="numeric", length=length(samplesize)),
  CH_CD = vector(mode="numeric", length=length(samplesize)),
  SH_CH = vector(mode="numeric", length=length(samplesize)),
  SD_CD = vector(mode="numeric", length=length(samplesize))
)

dice <- 1:d

p_SH = double_prob_low
p_SD = double_prob_low + (double_prob_high - double_prob_low) * partner_effect
p_CH = double_prob_low + (double_prob_high - double_prob_low) * (1-partner_effect)
p_CD = double_prob_high

start_time <- Sys.time()

for (N in samplesize) {
  
  runtime <- Sys.time() - start_time
  start_time <- Sys.time()
  message(paste("Sample size = ", N, ", Time: ", Sys.time(), ", runtime = ", runtime, sep=""))

  
  # Fill in parameters
  
  N_SH = N
  N_SD = N
  N_CH = N
  N_CD = N
  
  params <- matrix(c(N_SH, N_SD, N_CH, N_CD,
                     p_SH, p_SD, p_CH, p_CD), 
                   nrow=4, ncol=2, byrow = FALSE,
                   dimnames=list(c("SH", "SD", "CH", "CD"), c("N", "p"))
  )
  
  # Generate data frame for raw data
  
  IDs <- sample(10000:99999, sum(params[,"N"]), replace=FALSE)
  
  conditions <- c(rep("SH", params["SH","N"]), 
                  rep("SD", params["SD","N"]), 
                  rep("CH", params["CH","N"]), 
                  rep("CD", params["CD","N"]))
  
  games <- substr(conditions,1,1)
  
  partners <- substr(conditions,2,2)
  
  fingerratios <- rnorm(sum(params[,"N"]), mean = 1, sd = 0.1)
  
  rawdata <- data.frame(
    ID = rep(IDs, times = 1, length.out = NA, each = t), 
    Condition = rep(conditions, times = 1, length.out = NA, each = t),
    Game = rep(games, times = 1, length.out = NA, each = t), 
    Partner = rep(partners, times = 1, length.out = NA, each = t), 
    Index = rep(1:t, sum(params[,"N"])), 
    ValueA = vector("integer", sum(params[,"N"])*t), 
    ValueB = vector("integer", sum(params[,"N"])*t), 
    Double = vector("logical", sum(params[,"N"])*t),
    Fingerratio = rep(fingerratios, times = 1, length.out = NA, each = t),
    Q1 = sample(letters, sum(params[,"N"])*t, replace = TRUE), 
    Q2 = sample(letters, sum(params[,"N"])*t, replace = TRUE))
  
  # Run simulations
  
  counter <- rep(0, 8)
  
  for (s in 1:simulations){
    
    #message(paste("Sample size = ", N, "; Simulation #", s, " runtime = ", runtime, sep=""))
    
    # Player A
    for (i in 1:sum(params[,"N"])) {
      if (partners[i] == "H") {rawdata$ValueA[((i-1)*t+1) : (i*t)] <- sample(dice, t, replace=TRUE)}
      if (partners[i] == "D") {rawdata$ValueA[((i-1)*t+1) : (i*t)] <- sample(a_T9_all_trials$PlayerA, t, replace=TRUE)}
    }
    
    # Player B
    be.double = c(
      runif(params["SH", "N"]*t, min = 0, max = 1) < params["SH", "p"],
      runif(params["SD", "N"]*t, min = 0, max = 1) < params["SD", "p"],
      runif(params["CH", "N"]*t, min = 0, max = 1) < params["CH", "p"],
      runif(params["CD", "N"]*t, min = 0, max = 1) < params["CD", "p"])
    
    for (i in 1:length(be.double)) {
      if (be.double[i]) {rawdata$ValueB[i] <- rawdata$ValueA[i]}
      else rawdata$ValueB[i] <- sample(dice[-rawdata$ValueA[i]], 1)
    }
    
    # Detect doubles
    rawdata <- mutate(rawdata, Double = ifelse(ValueA==ValueB, TRUE, FALSE))
    
    # Summary of participants
    participants <- rawdata %>%
      group_by(ID) %>%
      summarize(
        Condition = unique(Condition), 
        Nbof_doubles = sum(Double), 
        Avg_report = mean(ValueB))
    
    # Summary of groups
    groups <- participants %>%
      group_by(Condition) %>%
      arrange(desc(Condition)) %>%
      summarize(
        Nbof_participants = n_distinct(ID),
        Nbof_doubles_per_group = sum(Nbof_doubles),
        Avg_nbof_doubles = Nbof_doubles_per_group/Nbof_participants,
        Median_nbof_doubles = median(Nbof_doubles),
        Avg_avg_report = mean(Avg_report))
    groups <- arrange(groups, desc(Condition))
    
    # One sample Wilcoxon tests
    
    w1 <- list()
    for (i in groups$Condition) {
      current <- filter(participants, Condition == i)
      w1[[i]] <- wilcox.test(x=current$Nbof_doubles, mu=t/d, alternative = "greater")
    }
    
    results.wilcox.table1 <- data.frame(
      Condition = groups$Condition,
      p.value = c(w1[[1]]$p.value, w1[[2]]$p.value, w1[[3]]$p.value, w1[[4]]$p.value),
      W = c(w1[[1]]$statistic, w1[[2]]$statistic, w1[[3]]$statistic, w1[[4]]$statistic))

    # Two sample Wilcoxon tests
    
    w2 <- list()
    w2[[1]] <- wilcox.test(x=filter(participants, Condition=="SH")$Nbof_doubles, y=filter(participants, Condition=="SD")$Nbof_doubles, conf.int = TRUE, alternative = "less" )
    w2[[2]] <- wilcox.test(x=filter(participants, Condition=="CH")$Nbof_doubles, y=filter(participants, Condition=="CD")$Nbof_doubles, conf.int = TRUE, alternative = "less" )
    
    results.wilcox.table2 <- data.frame(
      Condition = c("SH-SD", "CH-CD"),
      p.value = c(w2[[1]]$p.value, w2[[2]]$p.value),
      W = c(w2[[1]]$statistic, w2[[2]]$statistic))
    
    w3 <- list()
    w3[[1]] <- wilcox.test(x=filter(participants, Condition=="SH")$Nbof_doubles, y=filter(participants, Condition=="CH")$Nbof_doubles, conf.int = TRUE, alternative = "less" )
    w3[[2]] <- wilcox.test(x=filter(participants, Condition=="SD")$Nbof_doubles, y=filter(participants, Condition=="CD")$Nbof_doubles, conf.int = TRUE, alternative = "less" )
    
    results.wilcox.table3 <- data.frame(
      Condition = c("SH-CH", "SD-CD"),
      p.value = c(w3[[1]]$p.value, w3[[2]]$p.value),
      W = c(w3[[1]]$statistic, w3[[2]]$statistic))
    
    # Count significant tests
    
    all_tests <- rbind(results.wilcox.table1, results.wilcox.table2, results.wilcox.table3)
    counter <- counter + as.numeric(all_tests$p.value < 0.05)
    
  }
  
  POWER <- (counter/s)*100
  POWERS[match(N,samplesize), 2:9] <- POWER
  
}

```

# Results

After simulating the reported values of all participants in this way, we calculated the statistical tests that we planned for our experiments and then calculated power for each test. The results of our simulations are shown in the table and figure below. The table shows the power of the one-sample Wilcoxon tests (columns 2-5) and the two-sample Wilcoxon tests (columns 6-9). We chose the sample size with which all tests had a power of at least 90%: .

```{r Save results}

# Table

kable(POWERS)

# Figure
# 
# plot(POWERS[,1], POWERS[,6], type="l")
# lines(POWERS[,1], POWERS[,7], col="blue")
# lines(POWERS[,1], POWERS[,8], col="red")
# lines(POWERS[,1], POWERS[,9], col="green")

# POWERS.df <- data.frame(
#   sample_size = rep(samplesize, times=8),
#   power = as.vector(t(POWERS[,2:9])),
#   test = rep(names(POWERS)[2:9], times=length(samplesize))
# )
# 
# powerplot <- ggplot(data=POWERS.df, aes(x=sample_size, y=power, col=test)) +
#   geom_line() 
# 
# print(powerplot)

# Save results and the last raw data

parameters <- list(
  random_seed = seed,
  tested_samplesizes = samplesize, # the number of participants in each group
  nbof_simulations_per_samplesize = simulations, # the number of simulations with one sample size
  nbof_trials = t, # the number of rounds (trials) each participant plays
  dice_sides = d, # the number of sides of the dice
  dishonestA_distribution = valueA_distr_high, # the distribution of reported numbers by simulated player As
  probof_doubles = c(p_SH, p_SD, p_CH, p_CD) # the probability of reporting doubles in each group
)

prefix <- format(seed, "%Y%m%d-%H%M%S")
save(rawdata, parameters, file=paste("C:/Users/fedor/OneDrive/Documents/DOKUMENTUMOK/Corruption.Project/GitHub/corruption_in_the_lab/power_simulations/RESULTS/", prefix, "_dummydata.RData", sep=""))
save(parameters, POWERS, file=paste("C:/Users/fedor/OneDrive/Documents/DOKUMENTUMOK/Corruption.Project/GitHub/corruption_in_the_lab/power_simulations/RESULTS/", prefix, "_MonteCarlo_results.RData", sep=""))

message(Sys.time())

```

